services:
  query_variant_categorization:
    container_name: query_variant_categorization
    image: heartexlabs/label-studio-ml-backend:llm-master
    init: true
    build:
      context: .
      args:
        TEST_ENV: ${TEST_ENV}
    environment:
      - MODEL_DIR=/data/models
      # Specify openai model provider: "openai", "azure", or "ollama"
      - OPENAI_PROVIDER=azure
      # Specify API key for openai or azure
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Specify model name for openai or azure (by default it uses "gpt-3.5-turbo")
      - OPENAI_MODEL=gpt-35-turbo
      # Internal prompt template for the model is:
      # **Source Text**:\n\n"{text}"\n\n**Task Directive**:\n\n"{prompt}"
      # if you want to specify task data keys in the prompt (i.e. input <TextArea name="$PROMPT_PREFIX..."/>, set this to 0
      - USE_INTERNAL_PROMPT_TEMPLATE=0
      # You can define the default prompt to be used before the user input
      # Can be the path to the file with the prompt or the prompt itself
      # ! Note that USE_INTERNAL_PROMPT_TEMPLATE should be set to 0 in this case
      # TODO: should be set to ./prompt.txt or /data/prompt.txt
      - DEFAULT_PROMPT=/data/prompt.txt
      # Prompt prefix for the TextArea component in the frontend to be used for the user input
      - PROMPT_PREFIX=prompt
      - RESPONSE_PREFIX=response
      # Log level for the server
      - LOG_LEVEL=DEBUG
      # Number of responses to generate for each request
      - NUM_RESPONSES=1
      # Temperature for the model
      - TEMPERATURE=0.7
      # Azure resource endpoint (in case OPENAI_PROVIDER=azure)
      - AZURE_RESOURCE_ENDPOINT=${AZURE_RESOURCE_ENDPOINT}
      # Azure deployment name (in case OPENAI_PROVIDER=azure)
      - AZURE_DEPLOYMENT_NAME=gpt-35-turbo
      # Azure API version (in case OPENAI_PROVIDER=azure)
      - AZURE_API_VERSION=2024-12-01-preview
      # Ollama Endpoint (in case OPENAI_PROVIDER=ollama, OPENAI_MODEL=<your_ollama_model>)
      # If running Ollama locally OLLAMA_ENDPOINT=http://host.docker.internal:11434/v1/
      - OLLAMA_ENDPOINT=
      # specify these parameters if you want to use basic auth for the model server
      - BASIC_AUTH_USER=
      - BASIC_AUTH_PASS=
      # specify the tags to be used for the model
      - TERM_REGION_TAG=term
      - CATEGORY_LABEL_TAG=label
      - CONFIDENCE_RATING_TAG=confidence
      - COMMENTS_TEXT_AREA_TAG=comments
    ports:
      # Expose the server port, format: "<host_port>:<container_port>"
      # For example, "9091:9090" exposes the server on port 9091 on the host machine
      # and port 9090 inside the container.
      # You can access the server at http://localhost:9091
      - "9091:9090"
    develop:
      watch:
        - action: rebuild
          path: ./
          target: ./
    volumes:
      - "./data/server:/data"